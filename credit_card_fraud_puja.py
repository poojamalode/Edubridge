# -*- coding: utf-8 -*-
"""Credit_Card_Fraud_Puja.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19DwmP72uxcVP1_Y8vHiEqV1WERbwJ-Di
"""

#import the libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Load the Dataset
data = pd.read_csv("/content/Credit.csv")

#Explore the Dataset using Head()
data.head()

#Explore the dataset using info()
data.info()

#Explore the details about the dataset
data.shape

#Describe the Dataset
data.describe()

#To check the count of Null values/NAN values
data.isnull().sum()

datac=data.copy()

#CountPlot to check Class is equally distributed or not
sns.countplot(x='Class',data=datac)
plt.title('Unequally Distributed Classes', fontsize=14)
plt.show()

fig, ax = plt.subplots(ncols=2, figsize=(10,4))
sns.boxplot(data=datac, x='Class', y="Amount", hue="Class", ax= ax[0], showfliers=True)
sns.boxplot(data=datac, x='Class', y="Amount", hue="Class", ax= ax[1], showfliers=False)

ax[0].set_title("Transaction Amount (Including Fliers)")
ax[1].set_title("Transaction Amount (Excluding Fliers)")

legend_labels = ["Non-Fruad","Fruad"]

for i in range(2):
    handles, _ = ax[i].get_legend_handles_labels()
    ax[i].legend(handles, legend_labels)

plt.show()

#Plot the Heatmap for correation
plt.figure(figsize=(14,9))
sns.heatmap(datac.corr(), annot=False, annot_kws={"size":6})
plt.show()

# To apply the Standard Scaler

from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
scaled_time = ss.fit_transform(datac['Time'].values.reshape(-1,1))
scaled_amount = ss.fit_transform(datac['Amount'].values.reshape(-1,1))

# Dropping the Time and Amount colums
time=datac.drop(['Time'], axis=1, inplace=True)
amount=datac.drop(['Amount'], axis=1, inplace=True)

# Inserting the new values of Time and Amount in DataSet
datac.insert(0, "Scaled Time",scaled_time )
datac.insert(1, "Scaled Amount", scaled_amount)

#Check the new added values using Head function
datac.head()

# Separating the Fraud and Non-Fraud values
fraud = datac.loc[data['Class']==1]
non_fraud = datac.loc[data['Class']==0][:492]

# Concatinating the Fraud and Non-Fraud values
data_EQ = pd.concat([fraud,non_fraud])

data_EQ.tail()

data_EQ.shape

#Count plot for Class getting Equally Distributed
sns.countplot(x='Class',data=data_EQ)
plt.title("Equally Distributed Classes", fontsize=14)
plt.show()

fig, ax = plt.subplots(ncols=2, figsize=(10,4))
sns.boxplot(data=data_EQ, x='Class', y="Scaled Amount", hue="Class", ax= ax[0], showfliers=True)
sns.boxplot(data=data_EQ, x='Class', y="Scaled Amount", hue="Class", ax= ax[1], showfliers=False)

ax[0].set_title("Transaction Amount (Including Fliers)")
ax[1].set_title("Transaction Amount (Excluding Fliers)")

legend_labels = ["Non-Fruad","Fruad"]

for i in range(2):
    a,b = ax[i].get_legend_handles_labels()
    ax[i].legend(a, legend_labels)
plt.show()

#Heatmap
plt.figure(figsize=(14,9))
sns.heatmap(data_EQ.corr(), annot=False, annot_kws={"size":6})
plt.show()

#Plotting the Subbox plot for all the Positive Correaltion
fig, axes = plt.subplots(ncols=4, figsize=(20,4))

sns.boxplot(x='Class', y="V2", data=data_EQ, ax=axes[0])
axes[0].set_title("V2 vs Class (+ve Correlation)")

sns.boxplot(x='Class', y='V4' , data=data_EQ, ax=axes[1])
axes[0].set_title("V3 vs Class (+ve Correlation)")

sns.boxplot(x='Class', y="V11", data=data_EQ, ax=axes[2])
axes[2].set_title("V11 vs Class (+ve Correlation)")

sns.boxplot(x='Class', y="V19", data=data_EQ, ax=axes[3])
axes[3].set_title("V19 vs Class (+ve Correlation)")

plt.show()

#Plotting the Subbox plot for all the Negative Correaltion
fig, axes = plt.subplots(ncols=4, figsize=(20,4))

sns.boxplot(x='Class', y="V3", data=data_EQ, ax=axes[0])
axes[0].set_title("V3 vs Class (-ve Correlation)")

sns.boxplot(x='Class', y="V10", data=data_EQ, ax=axes[1])
axes[1].set_title("V10 vs Class (-ve Correlation)")

sns.boxplot(x='Class', y="V12", data=data_EQ, ax=axes[2])
axes[2].set_title("V12 vs Class (-ve Correlation)")

sns.boxplot(x='Class', y="V14", data=data_EQ, ax=axes[3])
axes[3].set_title("V14 vs Class (-ve Correlation)")

plt.show()

#Function to Remove the Outliers
def remove_out(feature,df):
    fraud_array = df[feature].values
    q1 = np.percentile(fraud_array,25)
    q3 = np.percentile(fraud_array,75)
    iqr = q3 - q1

    minimum = q1 - (1.5 * iqr)
    maximum = q3 + (1.5 * iqr)

    new_data = df.drop(df[(df[feature] < minimum) | (df[feature] > maximum)].index,axis = 0,inplace = False)
    return new_data

# Calling the Remove Outliers fumction
new_data2 = remove_out('V2',data_EQ)
new_data2 = remove_out('V3',new_data2)
new_data2 = remove_out('V10',new_data2)

#Boxplot after removing the outliers
fig, axes = plt.subplots(ncols=3, figsize=(20,4))
sns.boxplot(x="Class", y="V2", data=new_data2, ax = axes[0])
axes[0].set_title('V2 vs Class (Positive Correlation)')

sns.boxplot(x="Class", y="V3", data=new_data2, ax = axes[1])
axes[1].set_title('V3 vs Class (Negative Correlation)')

sns.boxplot(x="Class", y="V10", data=new_data2, ax = axes[2])
axes[2].set_title('V10 vs Class (Negative Correlation)')

#Countplot to show data is getting equally distributed
sns.countplot(x='Class', data=new_data2)
plt.title("Newly Distributed Data", fontsize=14)
plt.show()

# Applying the TRAIN_TEST_SPLIT
from sklearn.model_selection import train_test_split
x=new_data2.drop('Class', axis=1)
y=new_data2['Class']
X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.3, random_state=1)

from sklearn.metrics import f1_score,precision_score, recall_score, accuracy_score

#1) BUILD THE Logistic Regression model
from sklearn.linear_model import LogisticRegression
lr=LogisticRegression()
lr.fit(X_train, y_train)

#Predict the data using Logistic Regression
y_pred=lr.predict(X_test)

f1 = f1_score(y_test,y_pred)
precision=precision_score(y_test,y_pred)
recall=recall_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)

print("F1 Score: ", f1)
print("Precision: ", precision)
print("Recall: ", recall)
print("Accuracy: ", accuracy)

#2)Build the Decision Tree Classifier Model
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier()
dtc.fit(X_train, y_train)

#Predict the data using DecisionTreeClassifier
y_pred_dtc=dtc.predict(X_test)

f1_dtc = f1_score(y_test,y_pred_dtc)
precision_dtc=precision_score(y_test,y_pred_dtc)
recall_dtc=recall_score(y_test, y_pred_dtc)
accuracy_dtc = accuracy_score(y_test, y_pred_dtc)
print("F1 Score: ", f1_dtc)
print("Precision: ", precision_dtc)
print("Recall: ", recall_dtc)
print("Accuracy: ", accuracy_dtc)

#3)Build the GaussianNB model
from sklearn.naive_bayes import GaussianNB
nb= GaussianNB()
nb.fit(X_train, y_train)

#Predict the data using GaussianNB model
y_pred_nb=dtc.predict(X_test)

f1_nb = f1_score(y_test,y_pred_nb)
precision_nb=precision_score(y_test,y_pred_nb)
recall_nb=recall_score(y_test, y_pred_nb)
accuracy_nb = accuracy_score(y_test, y_pred_nb)

print("F1 Score: ", f1_nb)
print("Precision: ", precision_nb)
print("Recall: ", recall_nb)
print("Accuracy: ", accuracy_nb)

CONCLUSION ON THIS PROJECT:


After applying all the three algorithms (1. Logistic Regression, 2. Decision Tree Classifier, 3. GussianNB)
we came to conclussion that the accuracy for all three algorithms are approximately same.
Hence, we can use any of the above mentioned algorithms.